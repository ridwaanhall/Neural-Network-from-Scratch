# Neural Network from Scratch - Usage Guide

## ğŸ¯ Complete MNIST Neural Network Implementation

This project provides a professional implementation of a neural network from scratch using only NumPy for MNIST digit classification.

## ğŸš€ Quick Start

### 1. Run the Complete Pipeline

```bash
# Full training with default settings
python main.py

# Quick test (reduced dataset and epochs)
python main.py --quick_test

# Use different architectures
python main.py --architecture deep --epochs 100

# Skip plot generation
python main.py --no_plots
```

### 2. Custom Training

```bash
# Custom training script
python train.py

# Custom evaluation
python test.py --model_path models/your_model.pkl
```

### 3. Minimal Testing

```bash
# Basic functionality test
python minimal_test.py

# Component testing
python test_basic.py
```

## ğŸ“ Project Structure

```txt
nn-scratch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_loader.py          # MNIST data loading and preprocessing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ activations.py          # Activation functions (ReLU, Sigmoid, etc.)
â”‚   â”‚   â”œâ”€â”€ layers.py               # Neural network layers (Dense, Dropout)
â”‚   â”‚   â””â”€â”€ neural_network.py       # Main NeuralNetwork class
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ loss_functions.py       # Loss functions (Cross-entropy, MSE, etc.)
â”‚   â”‚   â””â”€â”€ trainer.py              # Training logic with advanced features
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py              # Evaluation metrics
â”‚       â””â”€â”€ visualization.py        # Plotting utilities
â”œâ”€â”€ data/                           # MNIST data storage (auto-created)
â”œâ”€â”€ models/                         # Saved models (auto-created)
â”œâ”€â”€ logs/                           # Training logs and plots (auto-created)
â”œâ”€â”€ main.py                         # Complete end-to-end pipeline
â”œâ”€â”€ train.py                        # Standalone training script
â”œâ”€â”€ test.py                         # Standalone evaluation script
â”œâ”€â”€ minimal_test.py                 # Quick functionality test
â””â”€â”€ requirements.txt                # Dependencies
```

## ğŸ§  Model Architectures

### Simple (2 layers)

- Input (784) â†’ Dense(128, ReLU) â†’ Dense(10, Softmax)
- Fast training, good for testing

### Default (3 layers with dropout)

- Input (784) â†’ Dense(256, ReLU) â†’ Dropout(0.2) â†’ Dense(128, ReLU) â†’ Dropout(0.2) â†’ Dense(10, Softmax)
- Balanced performance and training time

### Deep (4 layers with dropout)

- Input (784) â†’ Dense(512, ReLU) â†’ Dropout(0.3) â†’ Dense(256, ReLU) â†’ Dropout(0.3) â†’ Dense(128, ReLU) â†’ Dropout(0.2) â†’ Dense(10, Softmax)
- Higher capacity, longer training time

## âš™ï¸ Features

### Core Neural Network

- âœ… **From-scratch implementation** using only NumPy
- âœ… **Multiple activation functions**: ReLU, Sigmoid, Tanh, Softmax, LeakyReLU
- âœ… **Multiple layer types**: Dense, Dropout
- âœ… **Weight initialization**: Xavier, He, Random
- âœ… **Backpropagation** with automatic gradient computation

### Training Features

- âœ… **Advanced optimizer**: SGD with momentum
- âœ… **Learning rate scheduling**: Step decay, exponential decay, plateau
- âœ… **Early stopping** with patience
- âœ… **Batch training** with configurable batch sizes
- âœ… **Multiple loss functions**: Cross-entropy, MSE, Binary cross-entropy, Huber

### Data & Evaluation

- âœ… **Automatic MNIST download** and preprocessing
- âœ… **Comprehensive metrics**: Accuracy, Precision, Recall, F1-Score
- âœ… **Confusion matrix** generation
- âœ… **Training visualization**: Loss/accuracy curves
- âœ… **Sample prediction visualization**

### Production Features

- âœ… **Model saving/loading** with pickle
- âœ… **Comprehensive logging** with timestamps
- âœ… **Progress tracking** during training
- âœ… **Error handling** and validation
- âœ… **Command-line interface** with arguments

## ğŸ“Š Expected Performance

- **Simple model**: ~85-90% accuracy in 10-20 epochs
- **Default model**: ~92-95% accuracy in 30-50 epochs  
- **Deep model**: ~95-97% accuracy in 50-100 epochs

## ğŸ”§ Configuration Options

### Command Line Arguments

```bash
python main.py [OPTIONS]

Options:
  --architecture {simple,default,deep}   Model architecture (default: default)
  --epochs INT                           Number of training epochs (default: 50)
  --batch_size INT                       Batch size (default: 128)
  --learning_rate FLOAT                  Learning rate (default: 0.001)
  --no_plots                            Skip plot generation
  --quick_test                          Run with reduced dataset for testing
```

### Programming Interface

```python
from src.models.neural_network import NeuralNetwork
from src.models.layers import DenseLayer, DropoutLayer
from src.models.activations import ReLU, Softmax

# Create custom model
model = NeuralNetwork()
model.add_layer(DenseLayer(784, 256, activation=ReLU(), weight_init='xavier'))
model.add_layer(DropoutLayer(0.2))
model.add_layer(DenseLayer(256, 10, activation=Softmax(), weight_init='xavier'))

# Train with custom settings
from src.training.trainer import Trainer
from src.training.loss_functions import CrossEntropyLoss

trainer = Trainer(
    model=model,
    loss_function=CrossEntropyLoss(),
    learning_rate=0.001,
    batch_size=128,
    momentum=0.9
)

history = trainer.train(X_train, y_train, X_val, y_val, epochs=50)
```

## ğŸ› Troubleshooting

### Common Issues

1. **Import errors**: Make sure you're running from the project root directory
2. **Memory issues**: Use smaller batch sizes or the `--quick_test` flag
3. **Slow training**: Use the simple architecture or fewer epochs for testing
4. **Missing plots**: Install matplotlib with `pip install matplotlib`

### Debug Mode

```python
# Enable detailed logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Run minimal test first
python minimal_test.py
```

## ğŸ“ˆ Extending the Implementation

### Add New Activation Functions

```python
# In src/models/activations.py
class YourActivation(ActivationFunction):
    def forward(self, x):
        # Your forward implementation
        pass
    
    def backward(self, x):
        # Your backward implementation  
        pass
```

### Add New Layer Types

```python
# In src/models/layers.py
class YourLayer(Layer):
    def forward(self, x, training=True):
        # Your forward implementation
        pass
    
    def backward(self, grad_output):
        # Your backward implementation
        pass
```

## ğŸ“ Notes

- **Pure NumPy**: No TensorFlow, PyTorch, or scikit-learn used for core ML operations
- **Educational**: Designed for learning neural network fundamentals
- **Production-ready**: Includes proper error handling, logging, and validation
- **Extensible**: Clean OOP design for easy modification and extension

## ğŸ“ Learning Outcomes

By studying this implementation, you'll understand:

- How neural networks work mathematically
- Backpropagation algorithm implementation  
- Gradient descent optimization
- Weight initialization strategies
- Regularization techniques (dropout)
- Training best practices
- Model evaluation and metrics
- Software engineering for ML projects
