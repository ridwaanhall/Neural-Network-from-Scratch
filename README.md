# Neural Network from Scratch - Professional Implementation

[![Python](https://img.shields.io/badge/Python-3.12%2B-blue.svg)](https://python.org)
[![NumPy](https://img.shields.io/badge/NumPy-Only-green.svg)](https://numpy.org)
[![Accuracy](https://img.shields.io/badge/MNIST%20Accuracy-96.71%25-brightgreen.svg)](/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)](/)

A complete, professional neural network implementation built entirely from scratch using only NumPy for MNIST digit classification. This project achieves **96.71% test accuracy** with a clean, object-oriented architecture and comprehensive documentation.

## üéØ Key Features

- ‚úÖ **Pure NumPy Implementation** - No TensorFlow, PyTorch, or scikit-learn for ML core
- ‚úÖ **96.71% Test Accuracy** - Proven performance on MNIST dataset  
- ‚úÖ **Professional Architecture** - Clean OOP design with separation of concerns
- ‚úÖ **5 Activation Functions** - ReLU, Sigmoid, Tanh, Softmax, LeakyReLU
- ‚úÖ **4 Loss Functions** - CrossEntropy, MSE, BCE, Huber Loss
- ‚úÖ **Advanced Training** - SGD with momentum, learning rate scheduling, early stopping
- ‚úÖ **Interactive GUI** - Real-time digit recognition with drawing canvas
- ‚úÖ **Organized Visualizations** - Timestamped directories for train/test/main runs
- ‚úÖ **Command-Line Interface** - Full CLI support with configurable parameters
- ‚úÖ **Model Persistence** - Save/load trained models with full state restoration

## üöÄ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Quick test (recommended first run)
python apps/main.py --epochs 2 --quick_test

# 3. Training with custom parameters
python apps/train.py --epochs 10 --batch-size 64 --learning-rate 0.001

# 4. Test a trained model
python apps/test.py --model-path models/mnist_model_YYYYMMDD_HHMMSS.pkl

# 5. Launch interactive GUI
python apps/play_app.py
```

## üìä Performance Results

Our implementation achieves **96.71% accuracy** on the MNIST test set with organized visualization outputs:

**Training Run Visualizations** (`logs/run_train_YYYYMMDD_HHMMSS/`):

- Training history plots with loss and accuracy curves  
- Confusion matrices (standard and normalized)
- Sample predictions with confidence scores
- Class distribution analysis
- Weight distribution visualizations

**Test Run Visualizations** (`logs/run_test_YYYYMMDD_HHMMSS/`):

- Model evaluation metrics and confusion matrices
- Error analysis with misclassification patterns
- Test-specific performance visualizations

## üìÅ Project Architecture

```txt
Neural-Network-from-Scratch/
‚îú‚îÄ‚îÄ üìÅ src/                             # Core Implementation
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_loader.py              # MNIST data pipeline
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ activations.py              # 5 activation functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layers.py                   # Dense & Dropout layers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ neural_network.py           # Main NeuralNetwork class
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loss_functions.py           # 4 loss functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                  # Advanced training system
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py                  # Evaluation metrics
‚îÇ       ‚îî‚îÄ‚îÄ visualization.py            # Professional plotting
‚îú‚îÄ‚îÄ üìÅ apps/                            # Main Apps
‚îÇ   ‚îú‚îÄ‚îÄ main.py                         # Complete pipeline
‚îÇ   ‚îú‚îÄ‚îÄ train.py                        # Standalone training
‚îÇ   ‚îú‚îÄ‚îÄ test.py                         # Model evaluation
‚îÇ   ‚îú‚îÄ‚îÄ demo.py                         # Component demo
‚îÇ   ‚îî‚îÄ‚îÄ play_app.py                     # Interactive GUI
‚îú‚îÄ‚îÄ üìÅ debug/
‚îÇ   ‚îú‚îÄ‚îÄ debug_model.py                  # Examine saved model structure
‚îÇ   ‚îî‚îÄ‚îÄ debug_test.py                   # Isolate the error in test.py
‚îú‚îÄ‚îÄ üìÅ test/
‚îÇ   ‚îú‚îÄ‚îÄ minimal_test.py                 # Verify the NN works with MNIST data
‚îÇ   ‚îú‚îÄ‚îÄ test_basic.py                   # Verify the NN implementation works
‚îÇ   ‚îî‚îÄ‚îÄ test_model_loading.py           # Check model loading functionality
‚îú‚îÄ‚îÄ üìÅ data/                            # MNIST dataset
‚îú‚îÄ‚îÄ üìÅ models/                          # Saved models
‚îú‚îÄ‚îÄ üìÅ logs/                            # Training logs & organized visualizations
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ run_train_YYYYMMDD_HHMMSS/   # Training run outputs
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ run_test_YYYYMMDD_HHMMSS/    # Test run outputs  
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ run_main_YYYYMMDD_HHMMSS/    # Main pipeline outputs
‚îú‚îÄ‚îÄ üìÑ requirements.txt                 # Dependencies
‚îú‚îÄ‚îÄ üìÑ README.md                        # This file
‚îú‚îÄ‚îÄ üìÑ USAGE_GUIDE.md                   # Detailed usage
‚îî‚îÄ‚îÄ üìÑ PROJECT_STATUS.md                # Complete status
```

## üß† Core Components

### Neural Network Architecture

- **Flexible Model Building** - Add layers dynamically with `add_layer()`
- **Forward Propagation** - Optimized NumPy matrix operations
- **Backpropagation** - Automatic gradient computation with chain rule
- **Model Serialization** - Complete save/load functionality

### Activation Functions (5 Types)

```python
ReLU()          # Rectified Linear Unit
Sigmoid()       # Logistic activation  
Tanh()          # Hyperbolic tangent
Softmax()       # Probability distribution
LeakyReLU()     # Parameterized ReLU
```

### Loss Functions (4 Types)

```python
CrossEntropyLoss()    # Multiclass classification
MeanSquaredError()    # Regression tasks
BinaryCrossEntropy()  # Binary classification  
HuberLoss()           # Robust loss function
```

### Training Features

- **SGD with Momentum** - Accelerated optimization
- **Learning Rate Scheduling** - Adaptive learning rates
- **Early Stopping** - Prevent overfitting
- **Mini-batch Training** - Memory-efficient processing
- **Progress Tracking** - Real-time monitoring

## üéÆ Usage Examples

### Command Line Interface

```bash
# Basic training
python apps/main.py

# Quick test with reduced dataset
python apps/main.py --quick_test

# Deep architecture for maximum accuracy
python apps/main.py --architecture deep

# Custom parameters
python apps/main.py --epochs 100 --batch_size 64 --learning_rate 0.01

# Skip plot generation
python apps/main.py --no_plots

# Standalone scripts
python apps/train.py    # Training only
python apps/test.py     # Evaluation only
python apps/demo.py     # Component demonstration
```

### Interactive GUI Application

```bash
# Launch drawing interface for digit recognition
python apps/play_app.py
```

### Programmatic Interface

```python
from src.models.neural_network import NeuralNetwork
from src.models.layers import DenseLayer, DropoutLayer
from src.models.activations import ReLU, Softmax
from src.training.trainer import Trainer

# Create custom model
model = NeuralNetwork()
model.add_layer(DenseLayer(784, 256, activation=ReLU()))
model.add_layer(DropoutLayer(0.2))
model.add_layer(DenseLayer(256, 128, activation=ReLU()))
model.add_layer(DenseLayer(128, 10, activation=Softmax()))

# Train with advanced features
trainer = Trainer(model, patience=15, save_best=True)
history = trainer.train(X_train, y_train, X_val, y_val, 
                       epochs=50, batch_size=64, momentum=0.9)
```

## üèóÔ∏è Model Architectures

### Simple (Quick Testing)

```txt
Input(784) ‚Üí Dense(128, ReLU) ‚Üí Dense(10, Softmax)
Performance: 85-90% accuracy, 1-2 minutes training
```

### Default (Balanced)

```txt
Input(784) ‚Üí Dense(256, ReLU) ‚Üí Dropout(0.2) ‚Üí Dense(128, ReLU) ‚Üí Dropout(0.2) ‚Üí Dense(10, Softmax)
Performance: 92-95% accuracy, 3-5 minutes training
```

### Deep (Maximum Accuracy)

```txt  
Input(784) ‚Üí Dense(512, ReLU) ‚Üí Dropout(0.3) ‚Üí Dense(256, ReLU) ‚Üí Dropout(0.3) ‚Üí Dense(128, ReLU) ‚Üí Dropout(0.2) ‚Üí Dense(10, Softmax)
Performance: 95-97% accuracy, 8-15 minutes training
```

## üìã Requirements*

```txt
numpy>=2.2.6              # Core mathematical operations
torch>=2.7.1+cpu          # MNIST dataset downloading only
torchvision>=0.22.1+cpu   # Dataset utilities
matplotlib>=3.10.3        # Visualization (optional)
tkinter                   # GUI apps (standard library)
```

**Note: Core dependencies are listed above. See `requirements.txt` for the complete dependency list. Library versions may vary depending on your system and Python environment - the versions shown are from our development setup.*

## üèÜ Educational Value

This implementation demonstrates:

- **Mathematical Foundations**: Forward propagation, backpropagation, gradient descent
- **Software Engineering**: OOP design, modular architecture, error handling
- **Machine Learning**: Training strategies, regularization, evaluation metrics
- **Data Science**: Visualization, performance analysis, model interpretation

## üìÑ Documentation

- **README.md** - Project overview and quick start (this file)
- **USAGE_GUIDE.md** - Comprehensive usage instructions
- **PROJECT_STATUS.md** - Detailed project status and implementation details

## üöÄ Getting Started

1. **Clone and Setup**:

   ```bash
   git clone <repository-url>
   cd Neural-Network-from-Scratch
   pip install -r requirements.txt
   ```

2. **Quick Verification**:

   ```bash
   python test/minimal_test.py
   ```

3. **Run Demo**:

   ```bash
   python apps/demo.py
   ```

4. **Train Your First Model**:

   ```bash
   python apps/main.py --quick_test
   ```

5. **Full Training**:

   ```bash
   python apps/main.py
   ```

## üéØ Project Status

‚úÖ **FULLY OPERATIONAL** - Production ready with 96.71% test accuracy achieved!

This implementation successfully demonstrates professional neural network development from scratch using only NumPy, achieving state-of-the-art results on MNIST digit classification.
