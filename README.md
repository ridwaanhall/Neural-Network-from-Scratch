# Neural Network from Scratch - Professional Implementation

[![Python](https://img.shields.io/badge/Python-3.12%2B-blue.svg)](https://python.org)
[![NumPy](https://img.shields.io/badge/NumPy-Only-green.svg)](https://numpy.org)
[![Accuracy](https://img.shields.io/badge/MNIST%20Accuracy-96.71%25-brightgreen.svg)](/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)](/)

A complete, professional neural network implementation built entirely from scratch using only NumPy for MNIST digit classification. This project achieves **96.71% test accuracy** with a clean, object-oriented architecture and comprehensive documentation.

## ğŸ¯ Key Features

- âœ… **Pure NumPy Implementation** - No TensorFlow, PyTorch, or scikit-learn for ML core
- âœ… **96.71% Test Accuracy** - Proven performance on MNIST dataset  
- âœ… **Professional Architecture** - Clean OOP design with separation of concerns
- âœ… **5 Activation Functions** - ReLU, Sigmoid, Tanh, Softmax, LeakyReLU
- âœ… **4 Loss Functions** - CrossEntropy, MSE, BCE, Huber Loss
- âœ… **Advanced Training** - SGD with momentum, learning rate scheduling, early stopping
- âœ… **Interactive GUI** - Real-time digit recognition with drawing canvas
- âœ… **Organized Visualizations** - Timestamped directories for train/test/main runs
- âœ… **Command-Line Interface** - Full CLI support with configurable parameters
- âœ… **Model Persistence** - Save/load trained models with full state restoration

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Quick test (recommended first run)
python apps/main.py --epochs 2 --quick_test

# 3. Training with custom parameters
python apps/train.py --epochs 10 --batch-size 64 --learning-rate 0.001

# 4. Test a trained model
python apps/test.py --model-path models/mnist_model_YYYYMMDD_HHMMSS.pkl

# 5. Launch interactive GUI
python apps/play_app.py
```

## ğŸ“Š Performance Results

Our implementation achieves **96.71% accuracy** on the MNIST test set with organized visualization outputs:

**Training Run Visualizations** (`logs/run_train_YYYYMMDD_HHMMSS/`):

- Training history plots with loss and accuracy curves  
- Confusion matrices (standard and normalized)
- Sample predictions with confidence scores
- Class distribution analysis
- Weight distribution visualizations

**Test Run Visualizations** (`logs/run_test_YYYYMMDD_HHMMSS/`):

- Model evaluation metrics and confusion matrices
- Error analysis with misclassification patterns
- Test-specific performance visualizations

## ğŸ“ Project Architecture

```txt
Neural-Network-from-Scratch/
â”œâ”€â”€ ğŸ“ src/                             # Core Implementation
â”‚   â”œâ”€â”€ ğŸ“ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_loader.py              # MNIST data pipeline
â”‚   â”œâ”€â”€ ğŸ“ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ activations.py              # 5 activation functions
â”‚   â”‚   â”œâ”€â”€ layers.py                   # Dense & Dropout layers
â”‚   â”‚   â””â”€â”€ neural_network.py           # Main NeuralNetwork class
â”‚   â”œâ”€â”€ ğŸ“ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loss_functions.py           # 4 loss functions
â”‚   â”‚   â””â”€â”€ trainer.py                  # Advanced training system
â”‚   â””â”€â”€ ğŸ“ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py                  # Evaluation metrics
â”‚       â””â”€â”€ visualization.py            # Professional plotting
â”œâ”€â”€ ğŸ“ apps/                            # Main Apps
â”‚   â”œâ”€â”€ main.py                         # Complete pipeline
â”‚   â”œâ”€â”€ train.py                        # Standalone training
â”‚   â”œâ”€â”€ test.py                         # Model evaluation
â”‚   â”œâ”€â”€ demo.py                         # Component demo
â”‚   â””â”€â”€ play_app.py                     # Interactive GUI
â”œâ”€â”€ ğŸ“ debug/
â”‚   â”œâ”€â”€ debug_model.py                  # Examine saved model structure
â”‚   â””â”€â”€ debug_test.py                   # Isolate the error in test.py
â”œâ”€â”€ ğŸ“ test/
â”‚   â”œâ”€â”€ minimal_test.py                 # Verify the NN works with MNIST data
â”‚   â”œâ”€â”€ test_basic.py                   # Verify the NN implementation works
â”‚   â””â”€â”€ test_model_loading.py           # Check model loading functionality
â”œâ”€â”€ ğŸ“ data/                            # MNIST dataset
â”œâ”€â”€ ğŸ“ models/                          # Saved models
â”œâ”€â”€ ğŸ“ logs/                            # Training logs & organized visualizations
â”‚   â”œâ”€â”€ ğŸ“ run_train_YYYYMMDD_HHMMSS/   # Training run outputs
â”‚   â”œâ”€â”€ ğŸ“ run_test_YYYYMMDD_HHMMSS/    # Test run outputs  
â”‚   â””â”€â”€ ğŸ“ run_main_YYYYMMDD_HHMMSS/    # Main pipeline outputs
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Dependencies
â”œâ”€â”€ ğŸ“„ README.md                        # This file
â”œâ”€â”€ ğŸ“„ USAGE_GUIDE.md                   # Detailed usage
â””â”€â”€ ğŸ“„ PROJECT_STATUS.md                # Complete status
```

## ğŸ§  Core Components

### Neural Network Architecture

- **Flexible Model Building** - Add layers dynamically with `add_layer()`
- **Forward Propagation** - Optimized NumPy matrix operations
- **Backpropagation** - Automatic gradient computation with chain rule
- **Model Serialization** - Complete save/load functionality

### Activation Functions (5 Types)

```python
ReLU()          # Rectified Linear Unit
Sigmoid()       # Logistic activation  
Tanh()          # Hyperbolic tangent
Softmax()       # Probability distribution
LeakyReLU()     # Parameterized ReLU
```

### Loss Functions (4 Types)

```python
CrossEntropyLoss()    # Multiclass classification
MeanSquaredError()    # Regression tasks
BinaryCrossEntropy()  # Binary classification  
HuberLoss()           # Robust loss function
```

### Training Features

- **SGD with Momentum** - Accelerated optimization
- **Learning Rate Scheduling** - Adaptive learning rates
- **Early Stopping** - Prevent overfitting
- **Mini-batch Training** - Memory-efficient processing
- **Progress Tracking** - Real-time monitoring

## ğŸ® Usage Examples

### Command Line Interface

```bash
# Basic training
python apps/main.py

# Quick test with reduced dataset
python apps/main.py --quick_test

# Deep architecture for maximum accuracy
python apps/main.py --architecture deep

# Custom parameters
python apps/main.py --epochs 100 --batch_size 64 --learning_rate 0.01

# Skip plot generation
python apps/main.py --no_plots

# Standalone scripts
python apps/train.py    # Training only
python apps/test.py     # Evaluation only
python apps/demo.py     # Component demonstration
```

### Interactive GUI Application

```bash
# Launch drawing interface for digit recognition
python apps/play_app.py
```

### Programmatic Interface

```python
from src.models.neural_network import NeuralNetwork
from src.models.layers import DenseLayer, DropoutLayer
from src.models.activations import ReLU, Softmax
from src.training.trainer import Trainer

# Create custom model
model = NeuralNetwork()
model.add_layer(DenseLayer(784, 256, activation=ReLU()))
model.add_layer(DropoutLayer(0.2))
model.add_layer(DenseLayer(256, 128, activation=ReLU()))
model.add_layer(DenseLayer(128, 10, activation=Softmax()))

# Train with advanced features
trainer = Trainer(model, patience=15, save_best=True)
history = trainer.train(X_train, y_train, X_val, y_val, 
                       epochs=50, batch_size=64, momentum=0.9)
```

## ğŸ—ï¸ Model Architectures

### Simple (Quick Testing)

```txt
Input(784) â†’ Dense(128, ReLU) â†’ Dense(10, Softmax)
Performance: 85-90% accuracy, 1-2 minutes training
```

### Default (Balanced)

```txt
Input(784) â†’ Dense(256, ReLU) â†’ Dropout(0.2) â†’ Dense(128, ReLU) â†’ Dropout(0.2) â†’ Dense(10, Softmax)
Performance: 92-95% accuracy, 3-5 minutes training
```

### Deep (Maximum Accuracy)

```txt  
Input(784) â†’ Dense(512, ReLU) â†’ Dropout(0.3) â†’ Dense(256, ReLU) â†’ Dropout(0.3) â†’ Dense(128, ReLU) â†’ Dropout(0.2) â†’ Dense(10, Softmax)
Performance: 95-97% accuracy, 8-15 minutes training
```

## ğŸ“‹ Requirements

```txt
numpy>=2.2.6              # Core mathematical operations
torch>=2.7.1+cpu          # MNIST dataset downloading only
torchvision>=0.22.1+cpu   # Dataset utilities
matplotlib>=3.10.3        # Visualization (optional)
tkinter                   # GUI apps (standard library)
```

## ğŸ† Educational Value

This implementation demonstrates:

- **Mathematical Foundations**: Forward propagation, backpropagation, gradient descent
- **Software Engineering**: OOP design, modular architecture, error handling
- **Machine Learning**: Training strategies, regularization, evaluation metrics
- **Data Science**: Visualization, performance analysis, model interpretation

## ğŸ“„ Documentation

- **README.md** - Project overview and quick start (this file)
- **USAGE_GUIDE.md** - Comprehensive usage instructions
- **PROJECT_STATUS.md** - Detailed project status and implementation details

## ğŸš€ Getting Started

1. **Clone and Setup**:

   ```bash
   git clone <repository-url>
   cd Neural-Network-from-Scratch
   pip install -r requirements.txt
   ```

2. **Quick Verification**:

   ```bash
   python test/minimal_test.py
   ```

3. **Run Demo**:

   ```bash
   python apps/demo.py
   ```

4. **Train Your First Model**:

   ```bash
   python apps/main.py --quick_test
   ```

5. **Full Training**:

   ```bash
   python apps/main.py
   ```

## ğŸ¯ Project Status

âœ… **FULLY OPERATIONAL** - Production ready with 96.71% test accuracy achieved!

This implementation successfully demonstrates professional neural network development from scratch using only NumPy, achieving state-of-the-art results on MNIST digit classification.
